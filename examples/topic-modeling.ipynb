{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.models.lmavitm import LMAVITM\n",
    "from contextualized_topic_models.utils.data_preparation import to_bow\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from contextualized_topic_models.datasets.dataset import LMTopicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = os.path.join('../contextualized_topic_models/data/gnews', 'vocab.pkl')\n",
    "vocab = json.load(open(vocab, 'r'))\n",
    "idx2token = {v: k for (k, v) in vocab.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "train = np.load(os.path.join('../contextualized_topic_models/data/gnews', 'train.txt.pkl'), encoding='latin1', allow_pickle=True)\n",
    "train_bow = to_bow(train, vocab_size)\n",
    "\n",
    "with open(\"../contextualized_topic_models/data/gnews/bert_embeddings_gnews\", \"rb\") as filino:\n",
    "    train_bert = pickle.load(filino)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = LMTopicDataset(train_bow, train_bert, idx2token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100,)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/100]\tSamples: [11108/1110800]\tTrain Loss: 92.68798731418235\tTime: 0:00:01.647715\n",
      "Epoch: [2/100]\tSamples: [22216/1110800]\tTrain Loss: 80.98612715075846\tTime: 0:00:01.620539\n",
      "Epoch: [3/100]\tSamples: [33324/1110800]\tTrain Loss: 75.86123634148812\tTime: 0:00:01.620759\n",
      "Epoch: [4/100]\tSamples: [44432/1110800]\tTrain Loss: 72.60012332750046\tTime: 0:00:01.622098\n",
      "Epoch: [5/100]\tSamples: [55540/1110800]\tTrain Loss: 70.16168988742629\tTime: 0:00:01.615312\n",
      "Epoch: [6/100]\tSamples: [66648/1110800]\tTrain Loss: 68.47249703549807\tTime: 0:00:01.712650\n",
      "Epoch: [7/100]\tSamples: [77756/1110800]\tTrain Loss: 67.17721079578062\tTime: 0:00:01.643833\n",
      "Epoch: [8/100]\tSamples: [88864/1110800]\tTrain Loss: 66.62049095914824\tTime: 0:00:01.637501\n",
      "Epoch: [9/100]\tSamples: [99972/1110800]\tTrain Loss: 65.64779914743316\tTime: 0:00:01.658840\n",
      "Epoch: [10/100]\tSamples: [111080/1110800]\tTrain Loss: 65.0162089330325\tTime: 0:00:01.639084\n",
      "Epoch: [11/100]\tSamples: [122188/1110800]\tTrain Loss: 64.35951931286854\tTime: 0:00:01.598692\n",
      "Epoch: [12/100]\tSamples: [133296/1110800]\tTrain Loss: 64.09529889072066\tTime: 0:00:01.638713\n",
      "Epoch: [13/100]\tSamples: [144404/1110800]\tTrain Loss: 63.77406420379737\tTime: 0:00:01.662033\n",
      "Epoch: [14/100]\tSamples: [155512/1110800]\tTrain Loss: 63.525625270778946\tTime: 0:00:01.644012\n",
      "Epoch: [15/100]\tSamples: [166620/1110800]\tTrain Loss: 63.34050641113105\tTime: 0:00:01.661999\n",
      "Epoch: [16/100]\tSamples: [177728/1110800]\tTrain Loss: 63.17086305161539\tTime: 0:00:01.601875\n",
      "Epoch: [17/100]\tSamples: [188836/1110800]\tTrain Loss: 62.82884902596946\tTime: 0:00:01.640575\n",
      "Epoch: [18/100]\tSamples: [199944/1110800]\tTrain Loss: 62.66173217465734\tTime: 0:00:01.644794\n",
      "Epoch: [19/100]\tSamples: [211052/1110800]\tTrain Loss: 62.539203735835095\tTime: 0:00:01.645115\n",
      "Epoch: [20/100]\tSamples: [222160/1110800]\tTrain Loss: 62.35339641193284\tTime: 0:00:01.657360\n",
      "Epoch: [21/100]\tSamples: [233268/1110800]\tTrain Loss: 62.21541673758496\tTime: 0:00:01.651910\n",
      "Epoch: [22/100]\tSamples: [244376/1110800]\tTrain Loss: 62.10754900043887\tTime: 0:00:01.664016\n",
      "Epoch: [23/100]\tSamples: [255484/1110800]\tTrain Loss: 62.00986123282488\tTime: 0:00:01.653075\n",
      "Epoch: [24/100]\tSamples: [266592/1110800]\tTrain Loss: 62.11272272447223\tTime: 0:00:01.628056\n",
      "Epoch: [25/100]\tSamples: [277700/1110800]\tTrain Loss: 61.98878522412057\tTime: 0:00:01.636745\n",
      "Epoch: [26/100]\tSamples: [288808/1110800]\tTrain Loss: 61.85435890799424\tTime: 0:00:01.616096\n",
      "Epoch: [27/100]\tSamples: [299916/1110800]\tTrain Loss: 61.793037983252496\tTime: 0:00:01.656844\n",
      "Epoch: [28/100]\tSamples: [311024/1110800]\tTrain Loss: 61.577176350589106\tTime: 0:00:01.646211\n",
      "Epoch: [29/100]\tSamples: [322132/1110800]\tTrain Loss: 61.61457070284367\tTime: 0:00:01.577233\n",
      "Epoch: [30/100]\tSamples: [333240/1110800]\tTrain Loss: 61.670193972671726\tTime: 0:00:01.643601\n",
      "Epoch: [31/100]\tSamples: [344348/1110800]\tTrain Loss: 61.68915944413654\tTime: 0:00:01.659639\n",
      "Epoch: [32/100]\tSamples: [355456/1110800]\tTrain Loss: 61.45143267556322\tTime: 0:00:01.622323\n",
      "Epoch: [33/100]\tSamples: [366564/1110800]\tTrain Loss: 61.43202121007945\tTime: 0:00:01.653583\n",
      "Epoch: [34/100]\tSamples: [377672/1110800]\tTrain Loss: 61.2501618959168\tTime: 0:00:01.644093\n",
      "Epoch: [35/100]\tSamples: [388780/1110800]\tTrain Loss: 61.38914143030361\tTime: 0:00:01.618261\n",
      "Epoch: [36/100]\tSamples: [399888/1110800]\tTrain Loss: 61.29936001220967\tTime: 0:00:01.648243\n",
      "Epoch: [37/100]\tSamples: [410996/1110800]\tTrain Loss: 61.18799219347824\tTime: 0:00:01.643956\n",
      "Epoch: [38/100]\tSamples: [422104/1110800]\tTrain Loss: 61.298558928531236\tTime: 0:00:01.779840\n",
      "Epoch: [39/100]\tSamples: [433212/1110800]\tTrain Loss: 61.2914470250889\tTime: 0:00:01.678848\n",
      "Epoch: [40/100]\tSamples: [444320/1110800]\tTrain Loss: 61.3496121443301\tTime: 0:00:01.664195\n",
      "Epoch: [41/100]\tSamples: [455428/1110800]\tTrain Loss: 61.091401871820985\tTime: 0:00:01.663022\n",
      "Epoch: [42/100]\tSamples: [466536/1110800]\tTrain Loss: 61.08626981961199\tTime: 0:00:01.652949\n",
      "Epoch: [43/100]\tSamples: [477644/1110800]\tTrain Loss: 61.02140125113938\tTime: 0:00:01.638478\n",
      "Epoch: [44/100]\tSamples: [488752/1110800]\tTrain Loss: 61.13823959575869\tTime: 0:00:01.685017\n",
      "Epoch: [45/100]\tSamples: [499860/1110800]\tTrain Loss: 61.011408453155944\tTime: 0:00:01.669550\n",
      "Epoch: [46/100]\tSamples: [510968/1110800]\tTrain Loss: 61.096249430309236\tTime: 0:00:01.677184\n",
      "Epoch: [47/100]\tSamples: [522076/1110800]\tTrain Loss: 61.09640917230262\tTime: 0:00:01.684223\n",
      "Epoch: [48/100]\tSamples: [533184/1110800]\tTrain Loss: 61.03108524683786\tTime: 0:00:01.649244\n",
      "Epoch: [49/100]\tSamples: [544292/1110800]\tTrain Loss: 61.1051374853709\tTime: 0:00:01.674414\n",
      "Epoch: [50/100]\tSamples: [555400/1110800]\tTrain Loss: 60.87215005162383\tTime: 0:00:01.643505\n",
      "Epoch: [51/100]\tSamples: [566508/1110800]\tTrain Loss: 60.950207005227085\tTime: 0:00:01.668618\n",
      "Epoch: [52/100]\tSamples: [577616/1110800]\tTrain Loss: 60.871434069898946\tTime: 0:00:01.658398\n",
      "Epoch: [53/100]\tSamples: [588724/1110800]\tTrain Loss: 60.948663433378535\tTime: 0:00:01.640751\n",
      "Epoch: [54/100]\tSamples: [599832/1110800]\tTrain Loss: 60.8897123518804\tTime: 0:00:01.643339\n",
      "Epoch: [55/100]\tSamples: [610940/1110800]\tTrain Loss: 60.8229256047162\tTime: 0:00:01.647513\n",
      "Epoch: [56/100]\tSamples: [622048/1110800]\tTrain Loss: 60.84591209952849\tTime: 0:00:01.646239\n",
      "Epoch: [57/100]\tSamples: [633156/1110800]\tTrain Loss: 60.749416242797984\tTime: 0:00:01.681553\n",
      "Epoch: [58/100]\tSamples: [644264/1110800]\tTrain Loss: 60.914843890664386\tTime: 0:00:01.689846\n",
      "Epoch: [59/100]\tSamples: [655372/1110800]\tTrain Loss: 60.889142397371266\tTime: 0:00:01.645413\n",
      "Epoch: [60/100]\tSamples: [666480/1110800]\tTrain Loss: 60.795035329618855\tTime: 0:00:01.574420\n",
      "Epoch: [61/100]\tSamples: [677588/1110800]\tTrain Loss: 60.67500249679285\tTime: 0:00:01.639332\n",
      "Epoch: [62/100]\tSamples: [688696/1110800]\tTrain Loss: 60.69690631540894\tTime: 0:00:01.662709\n",
      "Epoch: [63/100]\tSamples: [699804/1110800]\tTrain Loss: 60.69282309202827\tTime: 0:00:01.696269\n",
      "Epoch: [64/100]\tSamples: [710912/1110800]\tTrain Loss: 60.678238041417224\tTime: 0:00:01.686610\n",
      "Epoch: [65/100]\tSamples: [722020/1110800]\tTrain Loss: 60.6564911515068\tTime: 0:00:01.699364\n",
      "Epoch: [66/100]\tSamples: [733128/1110800]\tTrain Loss: 60.62153710656171\tTime: 0:00:01.638245\n",
      "Epoch: [67/100]\tSamples: [744236/1110800]\tTrain Loss: 60.6673805530783\tTime: 0:00:01.643116\n",
      "Epoch: [68/100]\tSamples: [755344/1110800]\tTrain Loss: 60.67757621548096\tTime: 0:00:01.636997\n",
      "Epoch: [69/100]\tSamples: [766452/1110800]\tTrain Loss: 60.7728946673428\tTime: 0:00:01.642963\n",
      "Epoch: [70/100]\tSamples: [777560/1110800]\tTrain Loss: 60.561441500495135\tTime: 0:00:01.574486\n",
      "Epoch: [71/100]\tSamples: [788668/1110800]\tTrain Loss: 60.60438726065955\tTime: 0:00:01.719213\n",
      "Epoch: [72/100]\tSamples: [799776/1110800]\tTrain Loss: 60.58272270337257\tTime: 0:00:01.631984\n",
      "Epoch: [73/100]\tSamples: [810884/1110800]\tTrain Loss: 60.70107200328592\tTime: 0:00:01.680186\n",
      "Epoch: [74/100]\tSamples: [821992/1110800]\tTrain Loss: 60.68169605555962\tTime: 0:00:01.653674\n",
      "Epoch: [75/100]\tSamples: [833100/1110800]\tTrain Loss: 60.55795267206068\tTime: 0:00:01.661692\n",
      "Epoch: [76/100]\tSamples: [844208/1110800]\tTrain Loss: 60.59023686300133\tTime: 0:00:01.652096\n",
      "Epoch: [77/100]\tSamples: [855316/1110800]\tTrain Loss: 60.63777698575914\tTime: 0:00:01.620347\n",
      "Epoch: [78/100]\tSamples: [866424/1110800]\tTrain Loss: 60.53173757792807\tTime: 0:00:01.644843\n",
      "Epoch: [79/100]\tSamples: [877532/1110800]\tTrain Loss: 60.60319455853889\tTime: 0:00:01.667044\n",
      "Epoch: [80/100]\tSamples: [888640/1110800]\tTrain Loss: 60.64831455053509\tTime: 0:00:01.571885\n",
      "Epoch: [81/100]\tSamples: [899748/1110800]\tTrain Loss: 60.592876298332286\tTime: 0:00:01.589225\n",
      "Epoch: [82/100]\tSamples: [910856/1110800]\tTrain Loss: 60.50906463282375\tTime: 0:00:01.632152\n",
      "Epoch: [83/100]\tSamples: [921964/1110800]\tTrain Loss: 60.51901180279697\tTime: 0:00:01.654430\n",
      "Epoch: [84/100]\tSamples: [933072/1110800]\tTrain Loss: 60.557544833256436\tTime: 0:00:01.670527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [85/100]\tSamples: [944180/1110800]\tTrain Loss: 60.555617994913575\tTime: 0:00:01.657892\n",
      "Epoch: [86/100]\tSamples: [955288/1110800]\tTrain Loss: 60.31921703213619\tTime: 0:00:01.651721\n",
      "Epoch: [87/100]\tSamples: [966396/1110800]\tTrain Loss: 60.590828840278405\tTime: 0:00:01.632540\n",
      "Epoch: [88/100]\tSamples: [977504/1110800]\tTrain Loss: 60.28801789110326\tTime: 0:00:01.656925\n",
      "Epoch: [89/100]\tSamples: [988612/1110800]\tTrain Loss: 60.28068448521055\tTime: 0:00:01.663602\n",
      "Epoch: [90/100]\tSamples: [999720/1110800]\tTrain Loss: 60.29858714932369\tTime: 0:00:01.667879\n",
      "Epoch: [91/100]\tSamples: [1010828/1110800]\tTrain Loss: 60.41148183600783\tTime: 0:00:01.700559\n",
      "Epoch: [92/100]\tSamples: [1021936/1110800]\tTrain Loss: 60.532443229612106\tTime: 0:00:01.733382\n",
      "Epoch: [93/100]\tSamples: [1033044/1110800]\tTrain Loss: 60.49505933927124\tTime: 0:00:01.668005\n",
      "Epoch: [94/100]\tSamples: [1044152/1110800]\tTrain Loss: 60.51139240862442\tTime: 0:00:01.753200\n",
      "Epoch: [95/100]\tSamples: [1055260/1110800]\tTrain Loss: 60.395870726615954\tTime: 0:00:01.679055\n",
      "Epoch: [96/100]\tSamples: [1066368/1110800]\tTrain Loss: 60.398070278037224\tTime: 0:00:01.782549\n",
      "Epoch: [97/100]\tSamples: [1077476/1110800]\tTrain Loss: 60.39344523302462\tTime: 0:00:01.744075\n",
      "Epoch: [98/100]\tSamples: [1088584/1110800]\tTrain Loss: 60.35184426834826\tTime: 0:00:01.936433\n",
      "Epoch: [99/100]\tSamples: [1099692/1110800]\tTrain Loss: 60.32097006204706\tTime: 0:00:01.807627\n",
      "Epoch: [100/100]\tSamples: [1110800/1110800]\tTrain Loss: 60.37860276658714\tTime: 0:00:01.871701\n"
     ]
    }
   ],
   "source": [
    "lmavitm = LMAVITM(input_size=vocab_size, bert_input_size=len(train_bert[0]),  inferencetype=\"contextual\",\n",
    "                n_components=50, model_type=\"prodLDA\",\n",
    "              hidden_sizes=(100, ), activation='softplus', dropout=0.2,\n",
    "              learn_priors=True, batch_size=200, lr=2e-3, momentum=0.99,\n",
    "              solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
    "\n",
    "lmavitm.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['tax', 'irs', 'group', 'coalition', 'party'],\n",
       "             1: ['nsa', 'porn', 'habit', 'online', 'drug'],\n",
       "             2: ['light', 'hanukkah', 'rare', 'ison', 'comet'],\n",
       "             3: ['independence',\n",
       "              'scotland',\n",
       "              'scottish',\n",
       "              'independent',\n",
       "              'salmond'],\n",
       "             4: ['review', 'homefront', 'frozen', 'movie', 'disney'],\n",
       "             5: ['baldwin', 'alec', 'change', 'msnbc', 'climate'],\n",
       "             6: ['kobe', 'lakers', 'bryant', 'extension', 'contract'],\n",
       "             7: ['oldboy', 'spike', 'lee', 'remake', 'icahn'],\n",
       "             8: ['rise', 'risk', 'hewlett', 'packard', 'price'],\n",
       "             9: ['nuclear', 'deal', 'security', 'project', 'israel'],\n",
       "             10: ['china', 'air', 'zone', 'defense', 'sea'],\n",
       "             11: ['attack', 'kill', 'killed', 'texas', 'nurse'],\n",
       "             12: ['golden', 'won', 'frozen', 'globe', 'star'],\n",
       "             13: ['star', 'win', 'duck', 'dancing', 'live'],\n",
       "             14: ['xbox', 'microsoft', 'p', 'console', 'drive'],\n",
       "             15: ['report', 'seahawks', 'suspension', 'face', 'chief'],\n",
       "             16: ['nativity', 'black', 'watkins', 'ian', 'lostprophets'],\n",
       "             17: ['china', 'air', 'thai', 'zone', 'protest'],\n",
       "             18: ['thanksgiving', 'day', 'macy', 'parade', 'opening'],\n",
       "             19: ['baby', 'love', 'jennifer', 'hewitt', 'lawson'],\n",
       "             20: ['packer', 'illinois', 'lynch', 'break', 'niu'],\n",
       "             21: ['yankee', 'york', 'cano', 'net', 'beltran'],\n",
       "             22: ['pope', 'francis', 'call', 'reform', 'berlusconi'],\n",
       "             23: ['fda', 'andme', 'test', 'order', 'hot'],\n",
       "             24: ['year', 'three', 'collapse', 'held', 'mother'],\n",
       "             25: ['bank', 'book', 'men', 'jos', 'wearhouse'],\n",
       "             26: ['jellyfish', 'robot', 'sony', 'cheese', 'seahorse'],\n",
       "             27: ['rowe', 'wba', 'cluster', 'liverpool', 'bergeron'],\n",
       "             28: ['hp', 'moto', 'hewlett', 'packard', 'icahn'],\n",
       "             29: ['cowboy', 'week', 'raider', 'bay', 'play'],\n",
       "             30: ['swift', 'taylor', 'prince', 'william', 'bon'],\n",
       "             31: ['woman', 'morning', 'pill', 'work', 'facebook'],\n",
       "             32: ['comet', 'launch', 'ison', 'spacex', 'grand'],\n",
       "             33: ['typhoon', 'philippine', 'chelsea', 'despite', 'league'],\n",
       "             34: ['net', 'detroit', 'king', 'business', 'burger'],\n",
       "             35: ['case', 'aarushi', 'murder', 'guilty', 'talwar'],\n",
       "             36: ['kim', 'kanye', 'west', 'kardashian', 'bound'],\n",
       "             37: ['talk', 'group', 'syria', 'peace', 'obama'],\n",
       "             38: ['hiv', 'aid', 'greek', 'health', 'greece'],\n",
       "             39: ['storm', 'travel', 'weather', 'holiday', 'thanksgiving'],\n",
       "             40: ['nokia', 'lumia', 'tablet', 'inch', 'window'],\n",
       "             41: ['talk', 'peace', 'love', 'meningitis', 'princeton'],\n",
       "             42: ['video', 'franco', 'seth', 'kanye', 'james'],\n",
       "             43: ['report', 'hook', 'sandy', 'paper', 'long'],\n",
       "             44: ['bbm', 'android', 'coming', 'san', 'blackberry'],\n",
       "             45: ['google', 'view', 'street', 'comment', 'developer'],\n",
       "             46: ['friday', 'black', 'best', 'monday', 'deal'],\n",
       "             47: ['west', 'kanye', 'yeezus', 'kim', 'nike'],\n",
       "             48: ['flacco', 'joe', 'wildcat', 'raven', 'france'],\n",
       "             49: ['bronco', 'patriot', 'manning', 'brady', 'peyton']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmavitm.get_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contextualized_topic_models",
   "language": "python",
   "name": "contextualized_topic_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
